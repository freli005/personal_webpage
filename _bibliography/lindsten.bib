% Encoding: UTF-8


@STRING{a = {Automatica}}

@STRING{acc = {{IEEE} American Control Conference}}

@STRING{ap = {Academic Press}}

@STRING{assp = {{IEEE} Transactions on Acoustics, Speech and Signal Processing}}

@STRING{aw = {Addison-Wesley Publishing Company}}

@STRING{bh = {Birkh{\"a}user Verlag}}

@STRING{c = {Cambridge University Press}}

@STRING{collection_nips = {Advances in Neural Information Processing Systems ({NIPS})}}
@String{conf_acc                 = {American Control Conference ({ACC})}}

@STRING{conf_bmvc = {British Machine Vision Conference}}

@STRING{conf_aistats = {International Conference on Artificial Intelligence and Statistics}}

@STRING{conf_cdc = {{IEEE} Conference on Decision and Control ({CDC})}}

@STRING{conf_fusion = {International Conference on Information Fusion ({FUSION})}}

@STRING{conf_icassp = {{IEEE} International Conference on Acoustics, Speech and Signal Processing
	({ICASSP})}}

@STRING{conf_iccv = {{IEEE} International Conference on Computer Vision ({ICCV})}}

@STRING{conf_iclr = {International Conference on Learning Representations ({ICLR})}}

@STRING{conf_icml = {International Conference on Machine Learning ({ICML})}}

@STRING{conf_icra = {{IEEE} International Conference on Robotics and Automation ({ICRA})}}

@STRING{conf_ifacwc = {{IFAC} World Congress}}

@STRING{conf_ijcai = {International Joint Conference on Artificial Intelligence ({IJCAI})}}

@STRING{conf_mlsp = {{IEEE} International Workshop on Machine Learning for Signal Processing ({MLSP})}}

@STRING{conf_nips = {Conference on Neural Information Processing Systems ({NIPS})}}

@STRING{conf_safeprocess = {{IFAC} Symposium on Fault Detection, Supervision and Safety of Technical
	Processes (SafeProcess)}}

@STRING{conf_ssp = {{IEEE} Workshop on Statistical Signal Processing ({SSP})}}

@STRING{conf_sysid = {{IFAC} Symposium on System Identification ({SYSID})}}

@STRING{conf_uai = {Conference on Uncertainty in Artificial Intelligence ({UAI})}}

@STRING{cst = {{IEEE} Transactions on Control System Technology}}

@STRING{eusipco = {European Signal Processing Conference ({EUSIPCO})}}

@STRING{iccv = {{IEEE} International Conference on Computer Vision ({ICCV})}}

@STRING{ieeer = {{IEEE} Transactions on Robotics}}

@STRING{ieeera = {{IEEE} Transactions on Robotics and Automation}}

@STRING{ijc = {International Journal of Control}}

@STRING{ISYname = {Department of Electrical Engineering, Link\"oping University}}

@STRING{it = {{IEEE} Transactions on Information Theory}}

@STRING{jfr = {Journal of Field Robotics}}

@STRING{jour_aap = {The Annals of Applied Probability}}

@STRING{jour_ac = {{IEEE} Transactions on Automatic Control}}

@STRING{jour_ams = {Annals of Mathematical Statistics}}

@STRING{jour_as = {The Annals of Statistics}}

@STRING{jour_automatica = {Automatica}}

@STRING{jour_bernoulli = {Bernoulli}}

@STRING{jour_biometrika = {Biometrika}}

@STRING{jour_cst = {{IEEE} Transactions on Control Systems Technology}}

@STRING{jour_fntml = {Foundations and Trends in Machine Learning}}

@STRING{jour_jasa = {Journal of the American Statistical Association}}

@STRING{jour_jcgs = {Journal of Computational and Graphical Statistics}}

@STRING{jour_jmlr = {Journal of Machine Learning Research}}

@STRING{jour_jrssb = {Journal of the Royal Statistical Society: Series {B}}}

@STRING{jour_jrssc = {Journal of the Royal Statistical Society: Series {C}}}

@STRING{jour_proceedingsieee = {Proceedings of the {IEEE}}}

@STRING{jour_sjos = {Scandinavian Journal of Statistics}}

@STRING{jour_statcomp = {Statistics and Computing}}

@STRING{jour_tit = {{IEEE} Transactions on Information Theory}}

@STRING{jour_tsp = {{IEEE} Transactions on Signal Processing}}

@STRING{Lic = {Licentiate {T}hesis {N}o~}}

@STRING{LiU_addess = {{SE}-581 83 {L}ink\"oping, {S}weden}}

@STRING{LiU_PhD_School = {Link\"oping {S}tudies in {S}cience and {T}echnology}}

@STRING{MSc = {Master's {T}hesis {N}o~}}

@STRING{pami = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence}}

@STRING{PhDNo = {PhD thesis {N}o~}}

@STRING{pp = {Pergamon Press}}

@STRING{PROC = {Proceedings of the }}

@STRING{publisher_ap = {Academic Press}}

@STRING{publisher_artech = {Artech House}}

@STRING{publisher_cambridgepress = {Cambridge University Press}}

@STRING{publisher_jw = {John Wiley \& Sons}}

@STRING{publisher_mitpress = {{MIT} Press}}

@STRING{publisher_oxfordpress = {Oxford University Press}}

@STRING{publisher_pearson = {Pearson}}

@STRING{publisher_ph = {Prentice Hall}}

@STRING{publisher_springer = {Springer}}

@STRING{publisher_springverlag = {Springer-Verlag}}

@STRING{sae = {SAE Society of Automotive Engineers}}

@STRING{uai = {Conference on Uncertainty in Artificial Intelligence ({UAI})}}



@Misc{LindholmL:2019,
  author       = {Andreas Lindholm and Fredrik Lindsten},
  title        = {Learning dynamical systems with particle stochastic approximation {EM}},
  howpublished = {arXiv.org, arXiv:1806.09548},
  url = {https://arxiv.org/abs/1806.09548},
  year         = {2019},
  abstract = {We present the particle stochastic approximation EM (PSAEM) algorithm for learning of dynamical systems. The method builds on the EM algorithm, an iterative procedure for maximum likelihood inference in latent variable models. By combining stochastic approximation EM and particle Gibbs with ancestor sampling (PGAS), PSAEM obtains superior computational performance and convergence properties compared to plain particle-smoothing-based approximations of the EM algorithm. PSAEM can be used for plain maximum likelihood inference as well as for empirical Bayes learning of hyperparameters. Specifically, the latter point means that existing PGAS implementations easily can be extended with PSAEM to estimate hyperparameters at almost no extra computational cost. We discuss the convergence properties of the algorithm, and demonstrate it on several signal processing applications.},
  pid      = {WP2},
}









@InProceedings{KelviniusAPQAL:2025,
  author    = {Filip {Ekström Kelvinius} and Oskar B. Andersson and Abhijith S. Parackal and Dong Qian and Rickard Armiento and Fredrik Lindsten},
  title     = {WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry},
  booktitle = PROC #{ 42nd } # conf_icml,
  year      = {2025},
  abstract  = {Crystalline materials often exhibit a high level of symmetry. However, most generative models do not account for symmetry, but rather model each atom without any constraints on its position or element. We propose a generative model, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based descriptions of crystals. This is enabled by considering a crystal structure representation that encodes all symmetry, and we design a novel neural network architecture which enables using this representation inside a discrete generative model framework. In addition to respecting symmetry by construction, the discrete nature of our model enables fast generation. We additionally present a new metric, Fréchet Wrenformer Distance, which captures the symmetry aspects of the materials generated, and we benchmark WyckoffDiff against recently proposed generative models for crystal generation.},
  url = {https://arxiv.org/abs/2502.06485},
  note = {Forthcoming},
  pid   = {C67},
}

@InProceedings{KelviniusZL:2025,
  author    = {Ekström Kelvinius, Filip and Zhao, Zheng and Lindsten, Fredrik},
  title     = {Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled Diffusion Sequential Monte Carlo},
  booktitle = PROC #{ 42nd } # conf_icml,
  year      = {2025},
  abstract  = {A recent line of research has exploited pre-trained generative diffusion models as priors for solving Bayesian inverse problems. We contribute to this research direction by designing a sequential Monte Carlo method for linear-Gaussian inverse problems which builds on ``decoupled diffusion", where the generative process is designed such that larger updates to the sample are possible. The method is asymptotically exact and we demonstrate the effectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC) algorithm on both synthetic data and image reconstruction tasks. Further, we demonstrate how the approach can be extended to discrete data.},
  url = {https://arxiv.org/abs/2502.06379},
  note = {Forthcoming},
  pid   = {C66},
}

@InProceedings{DingAAULE:2025,
  author    = {Yifan Ding and Arturas Aleksandrauskas and Amirhossein Ahmadian and Jonas Unger and Fredrik Lindsten and Gabriel Eilertsen},
  title     = {Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations},
  booktitle = PROC # {Scandinavian Conference on Image Analysis},
  year      = {2025},
  abstract  = {Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders.},
  url = {https://arxiv.org/abs/2504.07793},
  note = {Forthcoming},
  pid   = {C65},
}

@InProceedings{AndraeLOL:2024,
  author    = {Andrae, Martin and Landelius, Tomas and Oskarsson, Joel and Lindsten, Fredrik},
  title     = {Continuous Ensemble Weather Forecasting with Diffusion models},
  booktitle = PROC #{ 13th } # conf_iclr,
  year      = {2025},
  month     = apr,
  address   = {Singapore},
  abstract  = {Weather forecasting has seen a shift in methods from numerical simulations to data-driven systems. While initial research in the area focused on deterministic forecasting, recent works have used diffusion models to produce skillful ensemble forecasts. These models are trained on a single forecasting step and rolled out autoregressively. However, they are computationally expensive and accumulate errors for high temporal resolution due to the many rollout steps. We address these limitations with Continuous Ensemble Forecasting, a novel and flexible method for sampling ensemble forecasts in diffusion models. The method can generate temporally consistent ensemble trajectories completely in parallel, with no autoregressive steps. Continuous Ensemble Forecasting can also be combined with autoregressive rollouts to yield forecasts at an arbitrary fine temporal resolution without sacrificing accuracy. We demonstrate that the method achieves competitive results for global weather forecasting with good probabilistic properties.},
  url       = {https://openreview.net/forum?id=ePEZvQNFDW},
  pid   = {C64},
}

@InProceedings{DucrocqGWL:2025,
  author    = {Ducrocq, Gabriel and Grunewald, Lukas and Westenhoff, Sebastian and Lindsten, Fredrik},
  title     = {cryoSPHERE: Single-Particle HEterogeneous REconstruction from cryo EM},
  booktitle = PROC #{ 13th } # conf_iclr,
  year      = {2025},
  month     = apr,
  address   = {Singapore},
  abstract  = {The three-dimensional structure of proteins plays a crucial role in determining their function. Protein structure prediction methods, like AlphaFold, offer rapid access to a protein’s structure. However, large protein complexes cannot be reliably predicted, and proteins are dynamic, making it important to resolve their full conformational distribution. Single-particle cryo-electron microscopy (cryo-EM) is a powerful tool for determining the structures of large protein complexes. Importantly, the numerous images of a given protein contain underutilized information about conformational heterogeneity. These images are very noisy projections of the protein, and traditional methods for cryo-EM reconstruction are limited to recovering only one or a few consensus conformations. In this paper, we introduce cryoSPHERE, which is a deep learning method that uses a nominal protein structure (e.g., from AlphaFold) as input, learns how to divide it into segments, and moves these segments as approximately rigid bodies to fit the different conformations present in the cryo-EM dataset. This approach provides enough constraints to enable meaningful reconstructions of single protein structural ensembles. We demonstrate this with two synthetic datasets featuring varying levels of noise, as well as one real dataset. We show that cryoSPHERE is very resilient to the high levels of noise typically encountered in experiments, where we see consistent improvements over the current state-of-the-art for heterogeneous reconstruction.},
  url       = {https://openreview.net/forum?id=n8O0trhost},
  pid   = {C63},
}

@InCollection{OskarssonLDL:2024,
  author    = {Oskarsson, Joel and Landelius, Tomas, and Deisenroth, Marc Peter and Lindsten, Fredrik},
  title     = {Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 37 (NeurIPS 2024)},
  year      = {2024},
  month     = dec,
  address   = {Vancouver, Canada},
  abstract  = {In recent years, machine learning has established itself as a powerful tool for high-resolution weather forecasting. While most current machine learning models focus on deterministic forecasts, accurately capturing the uncertainty in the chaotic weather system calls for probabilistic modeling. We propose a probabilistic weather forecasting model called Graph-EFM, combining a flexible latent-variable formulation with the successful graph-based forecasting framework. The use of a hierarchical graph construction allows for efficient sampling of spatially coherent forecasts. Requiring only a single forward pass per time step, Graph-EFM allows for fast generation of arbitrarily large ensembles. We experiment with the model on both global and limited area forecasting. Ensemble forecasts from Graph-EFM achieve equivalent or lower errors than comparable deterministic models, with the added benefit of accurately capturing forecast uncertainty.},
  url = {https://openreview.net/forum?id=wTIzpqX121},
  comment = {NeurIPS Spotlight},
  pid   = {C62},
}

@InProceedings{GovindarajanSRL:2024,
  author       = {Hariprasath Govindarajan, Per Sidén, Jacob Roll, Fredrik Lindsten},
  title        = {On Partial Prototype Collapse in the DINO Family of Self-Supervised Methods},
  booktitle = PROC #{ 35th } # conf_bmvc, 
  year         = {2024},
  address   = {Glasgow, UK},
  month     = nov,  
  abstract = {A prominent self-supervised learning paradigm is to model the representations as clusters, or more generally as a mixture model. Learning to map the data samples to compact representations and fitting the mixture model simultaneously leads to the representation collapse problem. Regularizing the distribution of data points over the clusters is the prevalent strategy to avoid this issue. While this is sufficient to prevent full representation collapse, we show that a partial prototype collapse problem still exists in the DINO family of methods, that leads to significant redundancies in the prototypes. Such prototype redundancies serve as shortcuts for the method to achieve a marginal latent class distribution that matches the prescribed prior. We show that by encouraging the model to use diverse prototypes, the partial prototype collapse can be mitigated. We study the downstream impact of effective utilization of the prototypes during pre-training. We show that it enables the methods to learn more fine-grained clusters, encouraging more informative representations. We demonstrate that this is especially beneficial when pre-training on a long-tailed fine-grained dataset.},
  pid      = {C61},
}

@InProceedings{KelviniusL:2024,
  author       = {Filip {Ekström Kelvinius} and Fredrik Lindsten},
  title        = {Discriminator Guidance for Autoregressive Diffusion Models},
  booktitle = PROC #{ 27th } # conf_aistats, 
  year         = {2024},
  address   = {Valencia, Spain},
  month     = may,  
  abstract = {We introduce discriminator guidance in the setting of Autoregressive Diffusion Models. The use of a discriminator to guide a diffusion process has previously been used for continuous diffusion models, and in this work we derive ways of using a discriminator together with a pretrained generative model in the discrete case. First, we show that using an optimal discriminator will correct the pretrained model and enable exact sampling from the underlying data distribution. Second, to account for the realistic scenario of using a sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which iteratively takes the predictions from the discrimiator into account during the generation process. We test these approaches on the task of generating molecular graphs and show how the discriminator improves the generative performance over using only the pretrained model.},
  pid      = {C60},
}

@InProceedings{OlminLSL:2024,
  author       = {Amanda Olmin and Jakob Lindqvist and Lennart Svensson and Fredrik Lindsten},
  title        = {On the connection between Noise-Contrastive Estimation and Contrastive Divergence},
  booktitle = PROC #{ 27th } # conf_aistats, 
  year         = {2024},
  address   = {Valencia, Spain},
  month     = may,  
  abstract = {Noise-contrastive estimation (NCE) is a popular method for estimating unnormalised probabilistic models, such as energy-based models, which are effective for modelling complex data distributions. Unlike classical maximum likelihood (ML) estimation that relies on importance sampling (resulting in ML-IS) or MCMC resulting in contrastive divergence, CD), NCE uses a proxy criterion to avoid the need for evaluating an often intractable normalisation constant. Despite apparent conceptual differences, we show that two NCE criteria, ranking NCE (RNCE) and conditional NCE (CNCE), can be viewed as ML estimation methods. Specifically, RNCE is equivalent to ML estimation combined with conditional importance sampling, and both RNCE and CNCE are special cases of CD. These findings bridge the gap between the two method classes and allow us to apply techniques from the ML-IS and CD literature to NCE, offering several advantageous extensions.},
  url      = {https://arxiv.org/abs/2402.16688},
  pid      = {C59},
}

@InProceedings{AhmadianDEL:2024,
  author       = {Amirhossein Ahmadian and Yifan Ding and Gabriel Eilertsen and Fredrik Lindsten},
  title        = {Unsupervised Novelty Detection in Pretrained Representation Space with Locally Adapted Likelihood Ratio},
  booktitle = PROC #{ 27th } # conf_aistats, 
  year         = {2024},
  address   = {Valencia, Spain},
  month     = may,  
  abstract = {Detecting novelties given unlabeled examples of normal data is a challenging task in machine learning, particularly when the novel and normal categories are semantically close. Large deep models pretrained on massive datasets can provide a rich representation space in which the simple k-nearest neighbor distance works as a novelty measure. However, as we show in this paper, the basic k-NN method might fail in this setup due to ignoring the `local geometry' of the distribution over representations as well as the impact of irrelevant 'background features'. To address this, we propose a fully unsupervised novelty detection approach that integrates the flexibility of k-NN with a locally adapted scaling of dimensions based on the 'neighbors of nearest neighbor' and the idea of 'likelihood ratio' in pretrained (self-supervised) representation spaces. Our experiments with image data show the advantage of this method when off-the-shelf vision transformers (e.g., pretrained by DINO) are used as the feature extractor without any fine-tuning.},
  pid      = {C58},
}

@inproceedings{OskarssonLL:2023,
  title = {Graph-based Neural Weather Prediction for Limited Area Modeling},
  author = {Oskarsson, Joel and Landelius, Tomas and Lindsten, Fredrik},
  booktitle = {NeurIPS 2023 Workshop on Tackling Climate Change with Machine Learning},
  year = {2023},
  month = dec,
  url = {https://arxiv.org/abs/2309.17370},
  pid = {C57}
}

@InProceedings{LindqvistOSL:2023,
  author       = {Jakob Lindqvist and Amanda Olmin and Lennart Svensson and Fredrik Lindsten},
  title        = {Generalised Active Learning with Annotation Quality Selection},
  booktitle = PROC #{ 33rd } # conf_mlsp, 
  year         = {2023},
  address   = {Rome, Italy},
  month     = sep,  
  abstract = {In this paper we promote a general formulation of active learning (AL), wherein the typically binary decision to annotate a point or not is extended to selecting the qualities with which the points should be annotated. By linking the annotation quality to the cost of acquiring the label, we can trade a lower quality for a larger set of training samples, which may improve learning for the same annotation cost. To investigate this AL formulation, we introduce a concrete criterion, based on the mutual information (MI) between model parameters and noisy labels, for selecting annotation qualities for the entire dataset, before any labels are acquired. We illustrate the usefulness of our formulation with examples for both classification and regression and find that MI is a good candidate for a criterion, but its complexity limits its usefulness.},
  pid      = {C56},
}

@InProceedings{VargaKRRLR:2023,
  author       = {Johannes Varga and Emil Karlsson and Günther Raidl and Elina Rönnberg and Fredrik Lindsten and Tobias Rodemann},
  title        = {Speeding up Logic-Based Benders Decomposition by Strengthening Cuts with Graph Neural Networks},
  booktitle = PROC #{the 9th International Conference on Machine Learning, Optimization and Data science (LOD)}, 
  year         = {2023},
  address   = {Grasmere, UK},
  month     = sep,  
  pid      = {C55},
}

@InProceedings{GlaserWLG:2023,
  author       = {Pierre Glaser and David Widmann and Fredrik Lindsten and Arthur Gretton},
  title        = {Fast and Scalable Score-Based Kernel Calibration Tests},
  booktitle = PROC #{ 39th } # uai, 
  year         = {2023},
  address   = {Pittsburgh, USA},
  month     = aug,
  abstract  = {We introduce the Kernel Calibration Conditional Stein Discrepancy test (KCCSD test), a nonparametric, kernel-based test for assessing the calibration of probabilistic models with well-defined scores. In contrast to previous methods, our test avoids the need for possibly expensive expectation approximations while providing control over its type-I error. We achieve these improvements by using a new family of kernels for score-based probabilities that can be estimated without probability density samples, and by using a Conditional Goodness of Fit criterion for the KCCSD test’s U-statistic. We demonstrate the properties of our test on various synthetic settings.},
  url       = {https://proceedings.mlr.press/v216/glaser23a.html},
  pid       = {C54},  
  comment   = {Spotlight},
}

@InProceedings{AhmadianL:2023,
  author    = {Amirhossein Ahmadian and Fredrik Lindsten},
  title     = {Enhancing Representation Learning with Deep Classifiers in Presence of Shortcut},
  booktitle = PROC #{ 48th } # conf_icassp,
  address   = {Rhodes Island, Greece},
  year      = {2023},
  month 	= jun,
  abstract  = {A deep neural classifier trained on an upstream task can be leveraged to boost the performance of another classifier in a related downstream task through the representations learned in hidden layers. However, presence of a shortcut (easy-to-learn feature) in the upstream task can considerably impair the versatility of intermediate representations and, in turn, the downstream performance. In this paper, we propose a method to improve the representations learned by deep neural image classifiers in spite of shortcuts in upstream data. In our method, the upstream classification objective is augmented with a type of adversarial training where an auxiliary network, so called lens, fools the classifier by exploiting the shortcut in reconstructing the images. Empirical comparisons in self-supervised and transfer learning tasks on three shortcut-biased datasets suggest the advantages of our method in terms of downstream performance and/or training time.},
  pid   = {C53},
}

@InProceedings{GovindarajanSRL:2022,
  author       = {Hariprasath Govindarajan and Per Sidén and Jacob Roll and Fredrik Lindsten},
  title        = {{DINO} as a von {M}ises-{F}isher mixture model},
  booktitle = PROC #{ 11th } # conf_iclr,
  year         = {2023},
  abstract = {Self-distillation methods using Siamese networks are popular for self-supervised pre-training. DINO is one such method based on a cross-entropy loss between K-dimensional probability vectors, obtained by applying a softmax function to the dot product between representations and learnt prototypes. Given the fact that the learned representations are L2-normalized, we show that DINO can be interpreted as a mixture model of von Mises-Fisher components. With this interpretation, DINO assumes equal precision for all components when the prototypes are also L2-normalized. Using this insight we propose DINO-vMF, that adds appropriate normalization constants when computing the cluster assignment probabilities. Unlike DINO, DINO-vMF is stable also for the larger ViT-Base model with unnormalized prototypes. We show that the added flexibility of the mixture model is beneficial in terms of better image representations. The DINO-vMF pre-trained model consistently performs better than DINO on a range of downstream tasks.},
  url      = {https://openreview.net/forum?id=cMJo1FTwBTQ},
  pid      = {C52},
  comment     = {ICLR Notable Paper Top 25 %},
}

@InProceedings{OskarssonSL:2023,
  author       = {Joel Oskarsson and Per Sidén and Fredrik Lindsten},
  title        = {Temporal Graph Neural Networks for Irregular Data},
  booktitle = PROC #{ 26th } # conf_aistats, 
  year         = {2023},
  abstract = {This paper proposes a temporal graph neural network model for forecasting of graph-structured irregularly observed time series. Our TGNN4I model is designed to handle both irregular time steps and partial observations of the graph. This is achieved by introducing a time-continuous latent state in each node, following a linear Ordinary Differential Equation (ODE) defined by the output of a Gated Recurrent Unit (GRU). The ODE has an explicit solution as a combination of exponential decay and periodic dynamics. Observations in the graph neighborhood are taken into account by integrating graph neural network layers in both the GRU state update and predictive model. The time-continuous dynamics additionally enable the model to make predictions at arbitrary time steps. We propose a loss function that leverages this and allows for training the model for forecasting over different time horizons. Experiments on simulated data and real-world data from traffic and climate modeling validate the usefulness of both the graph structure and time-continuous dynamics in settings with irregular observations.},
  url      = {https://arxiv.org/abs/2302.08415},
  pid      = {C51},
}

@InProceedings{OlminLSL:2022,
  author       = {Amanda Olmin and Jakob Lindqvist and Lennart Svensson and Fredrik Lindsten},
  title        = {Active Learning with Weak Supervision for Gaussian Processes},
  booktitle = PROC #{ 29th International Conference on Neural Information Processing (ICONIP)}, 
  year         = {2022},
  abstract = {Annotating data for supervised learning can be costly. When the annotation budget is limited, active learning can be used to select and annotate those observations that are likely to give the most gain in model performance. We propose an active learning algorithm that, in addition to selecting which observation to annotate, selects the precision of the annotation that is acquired. Assuming that annotations with low precision are cheaper to obtain, this allows the model to explore a larger part of the input space, with the same annotation costs. We build our acquisition function on the previously proposed BALD objective for Gaussian Processes, and empirically demonstrate the gains of being able to adjust the annotation precision in the active learning loop.},
  url      = {https://arxiv.org/abs/2204.08335},
  pid      = {C50},
}


@InProceedings{OskarssonSL:2022,
  author       = {Joel Oskarsson and Per Sidén and Fredrik Lindsten},
  title        = {Scalable Deep {G}aussian {M}arkov Random Fields for General Graphs},
  booktitle = PROC #{ 39th } # conf_icml, 
  year         = {2022},
  abstract = {Machine learning methods on graphs have proven useful in many applications due to their ability to handle generally structured data. The framework of Gaussian Markov Random Fields (GMRFs) provides a principled way to define Gaussian models on graphs by utilizing their sparsity structure. We propose a flexible GMRF model for general graphs built on the multi-layer structure of Deep GMRFs, originally proposed for lattice graphs only. By designing a new type of layer we enable the model to scale to large graphs. The layer is constructed to allow for efficient training using variational inference and existing software frameworks for Graph Neural Networks. For a Gaussian likelihood, close to exact Bayesian inference is available for the latent field. This allows for making predictions with accompanying uncertainty estimates. The usefulness of the proposed model is verified by experiments on a number of synthetic and real world datasets, where it compares favorably to other both Bayesian and deep learning methods.},
  url      = {https://proceedings.mlr.press/v162/oskarsson22a.html},
  pid      = {C49},
}


@InProceedings{OlminL:2022,
    title={Robustness and reliability when training with noisy labels},
    author={Amanda Olmin and Fredrik Lindsten},
    booktitle = PROC #{ 25th } # conf_aistats,
    pages = 	 {922--942},
    editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
    volume = 	 {151},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {28--30 Mar},
    publisher =    {PMLR},
    pdf = 	 {https://proceedings.mlr.press/v151/olmin22a/olmin22a.pdf},
    url = 	 {https://proceedings.mlr.press/v151/olmin22a.html},
    year={2022},
    abstract = 	 { Labelling of data for supervised learning can be costly and time-consuming and the risk of incorporating label noise in large data sets is imminent. When training a flexible discriminative model using a strictly proper loss, such noise will inevitably shift the solution towards the conditional distribution over noisy labels. Nevertheless, while deep neural networks have proven capable of fitting random labels, regularisation and the use of robust loss functions empirically mitigate the effects of label noise. However, such observations concern robustness in accuracy, which is insufficient if reliable uncertainty quantification is critical. We demonstrate this by analysing the properties of the conditional distribution over noisy labels for an input-dependent noise model. In addition, we evaluate the set of robust loss functions characterised by noise-insensitive, asymptotic risk minimisers. We find that strictly proper and robust loss functions both offer asymptotic robustness in accuracy, but neither guarantee that the final model is calibrated. Moreover, even with robust loss functions, overfitting is an issue in practice. With these results, we aim to explain observed robustness of common training practices, such as early stopping, to label noise. In addition, we aim to encourage the development of new noise-robust algorithms that not only preserve accuracy but that also ensure reliability. },
    pid      = {C48},	
}


@InProceedings{GovindarajanLLORL:2021,
	title={Self-Supervised Representation Learning for Content Based Image Retrieval of Complex Scenes},
	author={Hariprasath Govindarajan and Peter Lindskog and Dennis Lundstr\"om and Amanda Olmin and Jacob Roll and Fredrik Lindsten},
	booktitle = {32nd IEEE Intelligent Vehicles Symposium -- Workshop on Data Driven Intelligent Vehicle Applications},
	year={2021},
	doi = {10.1109/IVWorkshops54471.2021.9669246},
	abstract = {Although Content Based Image Retrieval (CBIR) is an active research field, application to images simultaneously containing multiple objects has received limited research interest. For such complex images, it is difficult to precisely convey the query intention, to encode all the image aspects into one compact global feature representation and to unambiguously define label similarity or dissimilarity. Motivated by the recent success on many visual benchmark tasks, we propose a selfsupervised method to train a feature representation learning model. We propose usage of multiple query images, and use an attention based architecture to extract features from diverse image aspects that benefits from this. The method shows promising performance on road scene datasets, and, consistently improves when multiple query images are used instead of a single query image.}, 
	pid      = {C47},	
}


@InProceedings{AhmadianL:2021,
	title={Likelihood-free Out-of-Distribution Detection with Invertible Generative Models},
	author={Amirhossein Ahmadian and Fredrik Lindsten},
	booktitle= PROC #{ 30th } # conf_ijcai, 
	doi = {10.24963/ijcai.2021/292},
	year={2021},	
	abstract = {Likelihood of generative models has been used traditionally as a score to detect atypical (Out-of-Distribution, OOD) inputs. However, several recent studies have found this approach to be highly unreliable, even with invertible generative models, where computing likelihood is feasible. In this paper, we present a different framework for generative model--based OOD detection that employs the model in constructing a new representation space, instead of using it directly in computing typicality scores, emphasizing that the score function should be interpretable as the similarity between the input and training data in the new space. In practice, with a focus on invertible models, we propose to extract low-dimensional features (statistics) based on the model encoder and complexity of input images, and then use a One-Class SVM to score the data. Contrary to recently proposed OOD detection methods for generative models, our method does not require computing likelihood values. Consequently, it is much faster when using invertible models with iteratively approximated likelihood (e.g. iResNet), while it still has a performance competitive with other related methods.},
	pid      = {C46},	
}

@InProceedings{WidmannLZ:2021,
	title={Calibration tests beyond classification},
	author={David Widmann and Fredrik Lindsten and Dave Zachariah},
	booktitle=PROC #{ 9th } # conf_iclr,
	year={2021},
	url={https://openreview.net/forum?id=-bxf89v3Nx},
	abstract = {Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification.},
	pid      = {C45},	
}

@InCollection{NaessethLB:2020,
  author       = {Christian A. Naesseth and Fredrik Lindsten and David M. Blei},
  title        = {Markovian Score Climbing: Variational Inference with KL(p||q)},
  abstract     = {Modern variational inference (VI) uses stochastic gradients to avoid intractable expectations, enabling large-scale probabilistic inference in complex models. VI posits a family of approximating distributions q and then finds the member of that family that is closest to the exact posterior p. Traditionally, VI algorithms minimize the “exclusive Kullback-Leibler (KL)” KL(q||p), often for computational convenience. Recent research, however, has also focused on the “inclusive KL” KL(p||q), which has good statistical properties that makes it more appropriate for certain inference problems. This paper develops a simple algorithm for reliably minimizing the inclusive KL. Consider a valid Markov chain Monte Carlo (MCMC) method, a Markov chain whose stationary distribution is p. The algorithm we develop iteratively samples the chain, and then uses those samples to follow the score function of the variational approximation with a Robbins-Monro stepsize schedule. This method, which we call Markovian score climbing (MSC), converges to a local optimum of the inclusive KL. It does not suffer from the systematic errors inherent in existing methods, such as Reweighted Wake-Sleep and Neural Adaptive Sequential Monte Carlo, which lead to bias in their final estimates. In a variant that ties the variational approximation directly to the Markov chain, MSC further provides a new algorithm that melds VI and MCMC. We illustrate convergence on a toy model and demonstrate the utility of MSC on Bayesian probit regression for classification as well as a stochastic volatility model for financial data.},
  booktitle = {Advances in Neural Information Processing Systems 33},
  url = {https://proceedings.neurips.cc/paper/2020/hash/b20706935de35bbe643733f856d9e5d6-Abstract.html},  
  year         = {2020},   
  pid      = {C44},
}


@InProceedings{LindqvistOLS:2020,
  author       = {Jakob Lindqvist and Amanda Olmin and Fredrik Lindsten and Lennart Svensson},
  title        = {A general framework for ensemble distribution distillation},
  booktitle = PROC #{ 30th } # conf_mlsp, 
  url = {https://arxiv.org/abs/2002.11531},
  year         = {2020},
  address   = {Virtual Conference},
  month     = sep,  
  abstract = {Ensembles of neural networks have been shown to give better performance than single networks, both in terms of predictions and uncertainty estimation. Additionally, ensembles allow the uncertainty to be decomposed into aleatoric (data) and epistemic (model) components, giving a more complete picture of the predictive uncertainty. Ensemble distillation is the process of compressing an ensemble into a single model, often resulting in a leaner model that still outperforms the individual ensemble members. Unfortunately, standard distillation erases the natural uncertainty decomposition of the ensemble. We present a general framework for distilling both regression and classification ensembles in a way that preserves the decomposition. We demonstrate the desired behaviour of our framework and show that its predictive performance is on par with standard distillation.},
  pid      = {C43},
}

@InProceedings{SidenL:2020,
  author       = {Per Sidén and Fredrik Lindsten},
  title        = {Deep {G}aussian {M}arkov random fields},
  booktitle = PROC #{ 37th } # conf_icml, 
  url = {https://proceedings.mlr.press/v119/siden20a.html},
  year         = {2020},
  address   = {Virtual Conference},
  month     = jul,  
  abstract = {Gaussian Markov random fields (GMRFs) are probabilistic graphical models widely used in spatial statistics and related fields to model dependencies over spatial structures. We establish a formal connection between GMRFs and convolutional neural networks (CNNs). Common GMRFs are special cases of a generative model where the inverse mapping from data to latent variables is given by a 1-layer linear CNN. This connection allows us to generalize GMRFs to multi-layer CNN architectures, effectively increasing the order of the corresponding GMRF in a way which has favorable computational scaling. We describe how well-established tools, such as autodiff and variational inference, can be used for simple and efficient inference and learning of the deep GMRF. We demonstrate the flexibility of the proposed model and show that it outperforms the state-of-the-art on a dataset of satellite temperatures, in terms of prediction and predictive uncertainty.},
  pid      = {C42},
}

@InProceedings{KudlickaMSL:2020,
  author    = {Jan Kudlicka and Lawrence M. Murray and Thomas B. Schön and Fredrik Lindsten},
  title     = {Particle filter with rejection control and unbiased estimator of the marginal likelihood},
  booktitle = PROC #{ 45th } # conf_icassp,
  address   = {Barcelona, Spain},
  year      = {2020},
  month 	= dec,
  url 		= {https://arxiv.org/abs/1910.09527},
  abstract  = {We consider the combined use of resampling and partial rejection control in sequential Monte Carlo methods, also known as particle filters. While the variance reducing properties of rejection control are known, there has not been (to the best of our knowledge) any work on unbiased estimation of the marginal likelihood (also known as the model evidence or the normalizing constant) in this type of particle filters. Being able to estimate the marginal likelihood without bias is highly relevant for model comparison, computation of interpretable and reliable confidence intervals, and in exact approximation methods, such as particle Markov chain Monte Carlo. In the paper we present a particle filter with rejection control that enables unbiased estimation of the marginal likelihood.},
  pid   = {C41},
}


@InCollection{WigrenRML:2019,
  author    = {Wigren, Anna and Risuleo, Riccardo Sven and Murray, Lawrence and Lindsten, Fredrik},
  title     = {Parameter elimination in particle {G}ibbs sampling},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8916--8927},
  abstract  = {Bayesian inference in state-space models is challenging due to high-dimensional state trajectories. A viable approach is particle Markov chain Monte Carlo (PMCMC), combining MCMC and sequential Monte Carlo to form exact approximations'' to otherwise-intractable MCMC methods. The performance of the approximation is limited to that of the exact method. We focus on particle Gibbs (PG) and particle Gibbs with ancestor sampling (PGAS), improving their performance beyond that of the ideal Gibbs sampler (which they approximate) by marginalizing out one or more parameters. This is possible when the parameter(s) has a conjugate prior relationship with the complete data likelihood. Marginalization yields a non-Markov model for inference, but we show that, in contrast to the general case, the methods still scale linearly in time. While marginalization can be cumbersome to implement, recent advances in probabilistic programming have enabled its automation. We demonstrate how the marginalized methods are viable as efficient inference backends in probabilistic programming, and demonstrate with examples in ecology and epidemiology.},
  comment = {NeurIPS Oral},
  pid   = {C40},
  url       = {http://papers.nips.cc/paper/9094-parameter-elimination-in-particle-gibbs-sampling},
}

@InCollection{WidmannLZ:2019,
  author    = {Widmann, David and Lindsten, Fredrik and Zachariah, Dave},
  title     = {Calibration tests in multi-class classification: A unifying framework},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {12236--12246},
  abstract  = {In safety-critical applications a probabilistic model is usually required to be calibrated, i.e., to capture the uncertainty of its predictions accurately. In multi-class classification, calibration of the most confident predictions only is often not sufficient. We propose and study calibration measures for multi-class classification that generalize existing measures such as the expected calibration error, the maximum calibration error, and the maximum mean calibration error. We propose and evaluate empirically different consistent and unbiased estimators for a specific class of measures based on matrix-valued kernels. Importantly, these estimators can be interpreted as test statistics associated with well-defined bounds and approximations of the p-value under the null hypothesis that the model is calibrated, significantly improving the interpretability of calibration measures, which otherwise lack any meaningful unit or scale.},
  comment = {NeurIPS Spotlight},
  pid   = {C39},
  url       = {http://papers.nips.cc/paper/9392-calibration-tests-in-multi-class-classification-a-unifying-framework},
}

@InCollection{NemethLFH:2019,
  author    = {Nemeth, Christopher and Lindsten, Fredrik and Filippone, Maurizio and Hensman, James},
  title     = {Pseudo-Extended {M}arkov chain {M}onte {C}arlo},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {4314--4324},
  abstract  = {Sampling from posterior distributions using Markov chain Monte Carlo (MCMC) methods can require an exhaustive number of iterations, particularly when the posterior is multi-modal as the MCMC sampler can become trapped in a local mode for a large number of iterations. In this paper, we introduce the pseudo-extended MCMC method as a simple approach for improving the mixing of the MCMC sampler for multi-modal posterior distributions. The pseudo-extended method augments the state-space of the posterior using pseudo-samples as auxiliary variables. On the extended space, the modes of the posterior are connected, which allows the MCMC sampler to easily move between well-separated posterior modes. We demonstrate that the pseudo-extended approach delivers improved MCMC sampling over the Hamiltonian Monte Carlo algorithm on multi-modal posteriors, including Boltzmann machines and models with sparsity-inducing priors.},
  pid   = {C38},
  url       = {https://papers.nips.cc/paper/8683-pseudo-extended-markov-chain-monte-carlo},
}

@InProceedings{UmenbergerSL:2019,
  author    = {Jack Umenberger and Thomas B. Schön and Fredrik Lindsten},
  title     = {Bayesian identification of state-space models via adaptive thermostats},
  booktitle = PROC #{ 58th } # conf_cdc,
  year      = {2019},
  address   = {Nice, France},
  month     = dec,
  pid   = {C37},
}

@InProceedings{VaicenaviciusWALRS:2019,
  author    = {Juozas Vaicenavicius and David Widmann and Carl Andersson and Fredrik Lindsten and Jacob Roll and Thomas B. Schön},
  title     = {Evaluating model calibration in classification},
  booktitle = PROC #{ 22nd } # conf_aistats,
  year      = {2019},
  address   = {Naha, Okinawa, Japan},
  month     = apr,
  pid   = {C36},
}

@InCollection{LindstenHV:2018,
  author    = {Lindsten, Fredrik and Helske, Jouni and Vihola, Matti},
  title     = {Graphical model inference: Sequential {M}onte {C}arlo meets deterministic approximations},
  booktitle = {Advances in Neural Information Processing Systems 31},
  publisher = {Curran Associates, Inc.},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {8190--8200},
  comment      = {NeurIPS Spotlight},
  pid   = {C35},
  url       = {http://papers.nips.cc/paper/8041-graphical-model-inference-sequential-monte-carlo-meets-deterministic-approximations.pdf},
}

@InProceedings{RisuleoLH:2018,
  author    = {Riccardo S. Risuleo and Fredrik Lindsten and Håkan Hjalmarsson},
  title     = {Semi-parametric kernel-based identification of {W}iener systems},
  booktitle = PROC #{ 57th } # conf_cdc,
  year      = {2018},
  address   = {Miami Beach, FL, USA},
  month     = dec,
  pid   = {C34},
}

@InProceedings{WigrenML:2018,
  author    = {Anna Wigren and Lawrence Murray and Fredrik Lindsten},
  title     = {Improving the particle filter in high dimensions using conjugate artificial process noise},
  booktitle = PROC #{ 18th } # conf_sysid,
  year      = {2018},
  address   = {Stockholm, Sweden},
  month     = jul,
  pid   = {C33},
}

@InProceedings{SvenssonLS:2018,
  author    = {Andreas Svensson and Fredrik Lindsten and Thomas B. Schön},
  title     = {Learning Nonlinear State-Space Models Using Smooth Particle-Filter-Based Likelihood Approximations},
  booktitle = PROC #{ 18th } # conf_sysid,
  year      = {2018},
  address   = {Stockholm, Sweden},
  month     = jul,
  pid   = {C32},
}

@InProceedings{RainforthNLPMDW:2016,
  author    = {Tom Rainforth and Christian A. Naesseth and Fredrik Lindsten and Brooks Paige and Jan-Willem van de Meent and Arnaud Doucet and Frank Wood},
  title     = {Interacting Particle {M}arkov Chain {M}onte Carlo},
  booktitle = PROC #{ 33rd } # conf_icml,
  year      = {2016},
  address   = {New York, USA},
  month     = jun,
  url = {https://proceedings.mlr.press/v48/rainforth16.html},
  pid   = {C31},
}

@InProceedings{WagbergLS:2015,
  author    = {Johan Wågberg and Fredrik Lindsten and Thomas B. Schön},
  title     = {Bayesian nonparametric identification of piecewise affine {ARX} systems},
  booktitle = PROC #{ 17th } # conf_sysid,
  year      = {2015},
  address   = {Beijing, China},
  month     = oct,
  pid   = {C30},
}

@InProceedings{DahlinLS:2015a,
  author    = {Johan Dahlin and Fredrik Lindsten and Thomas B. Schön},
  title     = {Quasi-{N}ewton particle {M}etropolis-{H}astings applied to intractable likelihood models},
  booktitle = PROC #{ 17th } # conf_sysid,
  year      = {2015},
  address   = {Beijing, China},
  month     = oct,
  pid   = {C29},
}

@InProceedings{RiabizLG:2015,
  author    = {Marina Riabiz and Fredrik Lindsten and Simon J. Godsill},
  title     = {Pseudo-Marginal {MCMC} for Parameter Estimation in Alpha-Stable Distributions},
  booktitle = PROC #{ 17th } # conf_sysid,
  year      = {2015},
  address   = {Beijing, China},
  month     = oct,
  pid   = {C28},
}

@InProceedings{SchonLDWNSD:2105,
  author    = {Thomas B. Schön and Fredrik Lindsten and Johan Dahlin and Johan Wågberg and Christian A. Naesseth and Andreas Svensson and Liang Dai},
  title     = {Sequential {M}onte {C}arlo Methods for System Identification},
  booktitle = PROC #{ 17th } # conf_sysid,
  year      = {2015},
  address   = {Beijing, China},
  month     = oct,
  pid   = {C27},
}

@InProceedings{NaessethLS:2015,
  author       = {Naesseth, Christian A. and Lindsten, Fredrik and Sch\"on, Thomas B.},
  title        = {Nested Sequential {M}onte {C}arlo Methods},
  booktitle    = PROC #{ 32nd International Conference on Machine Learning (ICML)},
  year         = {2015},
  address      = {Lille, France},
  month        = jul,
  pid      = {C26},
  howpublished = {arXiv.org, arXiv:1502.02536},
  owner        = {lindsten},
  timestamp    = {2015.03.13},
}

@InProceedings{Lacoste-JulienLB:2015,
  author    = {Simon Lacoste-Julien and Fredrik Lindsten and Francis Bach},
  title     = {Sequential Kernel Herding: {F}rank-{W}olfe Optimization for Particle Filtering},
  booktitle = PROC #{ 18th } # conf_aistats,
  year      = {2015},
  address   = {San Diego, USA},
  month     = may,
  pid   = {C25},
}

@InProceedings{BunchLS:2015,
  author    = {Bunch, Pete and Lindsten, Fredrik and Singh, Sumeetpal S.},
  title     = {Particle {G}ibbs with refreshed backward simulation},
  booktitle = PROC #{ 40th } # conf_icassp,
  year      = {2015},
  address   = {Brisbane, Australia},
  month     = apr,
  pid   = {C24},
  owner     = {lindsten},
  timestamp = {2014.11.26},
}

@InCollection{NaessethLS:2014,
  author    = {Naesseth, Christian A. and Lindsten, Fredrik and Sch\"on, Thomas B.},
  title     = {Sequential {M}onte {C}arlo for graphical models},
  booktitle = collection_nips #{ 27},
  year      = {2014},
  pages     = {1862--1870},
  pid   = {C23},
  owner     = {lindsten},
  timestamp = {2014-02-18},
  url       = {https://papers.nips.cc/paper/5570-sequential-monte-carlo-for-graphical-models},
}

@InProceedings{NaessethLS:2014a,
  author    = {Christian A. Naesseth and Fredrik Lindsten and Thomas B. Schön},
  title     = {Capacity estimation of two-dimensional channels using Sequential {M}onte {C}arlo},
  booktitle = PROC #{ 2014 IEEE Information Theory Workshop (ITW)},
  year      = {2014},
  address   = {Hobart, Tasmania},
  month     = nov,
  pid   = {C22},
}

@InProceedings{SvenssonLS:2014,
  author    = {Svensson, Andreas and Lindsten, Fredrik and Sch\"on, Thomas B.},
  title     = {Identification of jump {M}arkov linear models using particle filters},
  booktitle = PROC #{ 53rd } # conf_cdc,
  year      = {2014},
  address   = {Los Angeles, USA},
  month     = dec,
  url = {https://arxiv.org/abs/1409.7287},
  pid   = {C21},
  owner     = {lindsten},
  timestamp = {2014.09.02},
}

@InProceedings{FrigolaLSR:2014,
  author    = {Frigola, Roger and Lindsten, Fredrik and Sch\"on, Thomas B. and Rasmussen, Carl E.},
  title     = {Identification of {G}aussian Process State-Space Models with Particle Stochastic Approximation {EM}},
  booktitle = PROC #{ 19th } # conf_ifacwc,
  year      = {2014},
  address   = {Cape Town, South Africa},
  month     = aug,
  pid   = {C20},
  owner     = {lindsten},
  timestamp = {2014.02.19},
}

@InProceedings{DahlinL:2014,
  author    = {Dahlin, Johan and Lindsten, Fredrik},
  title     = {Particle filter-based {G}aussian Process Optimisation for Parameter Inference},
  booktitle = PROC #{ 19th } # conf_ifacwc,
  year      = {2014},
  address   = {Cape Town, South Africa},
  month     = aug,
  pid   = {C19},
  owner     = {lindsten},
  timestamp = {2014.02.13},
}

@InProceedings{DahlinLS:2014,
  author    = {Dahlin, Johan and Lindsten, Fredrik and Sch\"on, Thomas B.},
  title     = {Second-order Particle {MCMC} for {B}ayesian Parameter Inference},
  booktitle = PROC #{ 19th } # conf_ifacwc,
  year      = {2014},
  address   = {Cape Town, South Africa},
  month     = aug,
  pid   = {C18},
  owner     = {lindsten},
  timestamp = {2014.02.13},
}

@InProceedings{GunnarssonLC:2014,
  author    = {Fredrik Gunnarsson and Fredrik Lindsten and Niclas Carlsson},
  title     = {Particle Filtering for Network-Based Positioning Terrestrial Radio Networks},
  booktitle = PROC #{ IET Conference on Data Fusion and Target Tracking},
  year      = {2014},
  address   = {Liverpool, UK},
  comment   = {ISIF Best Paper Award},
  pid   = {C17},
}

@InCollection{FrigolaLSR:2013,
  author    = {Frigola, Roger and Lindsten, Fredrik and Sch\"on, Thomas B. and Rasmussen, Carl E.},
  title     = {{B}ayesian Inference and Learning in {G}aussian Process State-Space Models with Particle {MCMC}},
  booktitle = collection_nips #{ 26},
  year      = {2013},
  month     = dec,
  pid   = {C16},
  owner     = {lindsten},
  timestamp = {2013.12.03},
}

@InProceedings{Lindsten:2013a,
  author    = {Lindsten, Fredrik},
  title     = {An efficient stochastic approximation {EM} algorithm using conditional particle filters},
  booktitle = PROC #{ 38th } # conf_icassp,
  year      = {2013},
  address   = {Vancouver, Canada},
  month     = may,
  pid   = {C15},
  owner     = {lindsten},
  timestamp = {2012.12.17},
}

@InProceedings{DahlinLS:2013,
  author    = {Dahlin, Johan and Lindsten, Fredrik and Sch\"on, Thomas B.},
  title     = {Particle {M}etropolis {H}astings using {L}angevin dynamics},
  booktitle = PROC #{ 38th } # conf_icassp,
  year      = {2013},
  address   = {Vancouver, Canada},
  month     = may,
  pid   = {C14},
  owner     = {lindsten},
  timestamp = {2013.06.17},
}

@InProceedings{LindstenBGS:2013,
  author    = {Lindsten, Fredrik and Bunch, Pete and Godsill, Simon J. and Sch\"on, Thomas B.},
  title     = {Rao-{B}lackwellized particle smoothers for mixed linear/nonlinear state-space models},
  booktitle = PROC #{ 38th } # conf_icassp,
  year      = {2013},
  address   = {Vancouver, Canada},
  month     = may,
  pid   = {C13},
  owner     = {lindsten},
  timestamp = {2012.12.08},
}

@InProceedings{TaghaviLSS:2013,
  author    = {Taghavi, Ehsan and Lindsten, Fredrik and Svensson, Lennart and Sch\"on, Thomas B.},
  title     = {Adaptive stopping for fast particle smoothing},
  booktitle = PROC #{ 38th } # conf_icassp,
  year      = {2013},
  address   = {Vancouver, Canada},
  month     = may,
  pid   = {C12},
  owner     = {lindsten},
  timestamp = {2012.12.08},
}

@InCollection{LindstenJS:2012,
  author    = {Lindsten, Fredrik and Jordan, Michael I. and Sch\"on, Thomas B.},
  title     = {Ancestor Sampling for Particle {G}ibbs},
  booktitle = collection_nips #{ 25},
  year      = {2012},
  editor    = {Bartlett, P. and Pereira, F. C. N. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  pages     = {2600--2608},
  pid   = {C11},
  owner     = {lindsten},
  timestamp = {2013.06.25},
}

@InProceedings{LindstenSJ:2012,
  author    = {Lindsten, Fredrik and Sch\"on, Thomas B. and Jordan, Michael I.},
  title     = {A semiparametric {B}ayesian approach to {W}iener system identification},
  booktitle = PROC #{ 16th } # conf_sysid,
  year      = {2012},
  address   = {Brussels, Belgium},
  month     = jul,
  pid   = {C10},
  owner     = {lindsten},
  timestamp = {2011.12.01},
}

@InProceedings{LindstenSS:2012,
  author    = {Lindsten, Fredrik and Sch\"on, Thomas B. and Svensson, Lennart},
  title     = {A non-degenerate {R}ao-{B}lack\-wellised particle filter for estimating static parameters in dynamical models},
  booktitle = PROC #{ 16th } # conf_sysid,
  year      = {2012},
  address   = {Brussels, Belgium},
  month     = jul,
  pid   = {C9},
  owner     = {lindsten},
  timestamp = {2013.06.17},
}

@InProceedings{DahlinLSW:2012,
  author    = {Dahlin, Johan and Lindsten, Fredrik and Sch\"on, Thomas B. and Wills, Adrian},
  title     = {Hierarchical {B}ayesian {ARX} models for robust inference},
  booktitle = PROC #{ 16th } # conf_sysid,
  year      = {2012},
  address   = {Brussels, Belgium},
  month     = jul,
  pid   = {C8},
  owner     = {lindsten},
  timestamp = {2013.06.17},
}

@InProceedings{WillsSLN:2012,
  author    = {Wills, Adrian and Sch\"on, Thomas B. and Lindsten, Fredrik and Ninness, Brett},
  title     = {Estimation of Linear Systems using a {G}ibbs Sampler},
  booktitle = PROC #{ 16th } # conf_sysid,
  year      = {2012},
  address   = {Brussels, Belgium},
  month     = jul,
  pid   = {C7},
  owner     = {lindsten},
  timestamp = {2011.12.01},
}

@InProceedings{LindstenS:2012,
  author    = {Lindsten, Fredrik and Sch\"{o}n, Thomas B.},
  title     = {On the use of backward simulation in the particle {G}ibbs sampler},
  booktitle = PROC #{ 37th } # conf_icassp,
  year      = {2012},
  address   = {Kyoto, Japan},
  month     = mar,
  pid   = {C6},
  owner     = {lindsten},
  timestamp = {2011.10.04},
}

@InProceedings{LindstenOL:2011,
  author    = {Lindsten, Fredrik and Ohlsson, Henrik and Ljung, Lennart},
  title     = {Clustering using sum-of-norms regularization; with application to particle filter output computation},
  booktitle = PROC #{ } # conf_ssp,
  year      = {2011},
  address   = {Nice, France},
  month     = jun,
  pid   = {C5},
  owner     = {lindsten},
  timestamp = {2011.02.07},
}

@InProceedings{LindstenSO:2011,
  author    = {Lindsten, Fredrik and Sch\"on, Thomas B. and Olsson, Jimmy},
  title     = {An explicit variance reduction expression for the {R}ao-{B}lackwellised particle filter},
  booktitle = PROC #{ 18th } # conf_ifacwc,
  year      = {2011},
  address   = {Milan, Italy},
  month     = aug,
  pid   = {C4},
  owner     = {lindsten},
  timestamp = {2011.03.17},
}

@InProceedings{LindstenS:2010a,
  author    = {Lindsten, Fredrik and Sch\"{o}n, Thomas B.},
  title     = {Identification of Mixed Linear/Nonlinear State-Space Models},
  booktitle = PROC #{ 49th } # conf_cdc,
  year      = {2010},
  address   = {Atlanta, USA},
  month     = dec,
  pid   = {C3},
  owner     = {lindsten},
  timestamp = {2010.07.06},
}

@InProceedings{LindstenCOTSG:2010,
  author    = {Lindsten, Fredrik and Callmer, Jonas and Ohlsson, Henrik and T{\"{o}}rnqvist, David and Sch{\"{o}}n, Thomas B. and Gustafsson, Fredrik},
  title     = {Geo-referencing for {UAV} Navigation using Environmental Classification},
  booktitle = PROC #{ } # conf_icra,
  year      = {2010},
  address   = {Anchorage, USA},
  month     = May,
  pid   = {C2},
}

@InProceedings{LindstenNG:2009,
  author    = {Lindsten, Fredrik and Nordlund, Per-Johan and Gustafsson, Fredrik},
  title     = {Conflict Detection Metrics for Aircraft Sense and Avoid Systems},
  booktitle = PROC #{7th } # conf_safeprocess,
  year      = {2009},
  address   = {Barcelona, Spain},
  month     = Jul,
  pid   = {C1},
}


@PhdThesis{Lindsten:2013,
  author    = {Lindsten, Fredrik},
  title     = {Particle Filters and Markov Chains for Learning of Dynamical Systems},
  school    = ISYname,
  year      = {2013},
  type      = {Linköping Studies in Science and Technology. Dissertations, No. 1530},
  address   = {SE-581 83 Link{\"o}ping, Sweden},
  month     = oct,
  pid   = {PhD},
}

@PhdThesis{Lindsten:2011,
  author    = {Lindsten, Fredrik},
  title     = {Rao-{B}lackwellised particle methods for inference and identification},
  school    = ISYname,
  year      = {2011},
  type      = {Licentiate Thesis no. 1480},
  address   = {SE-581 83 Link{\"o}ping, Sweden},
  month     = jun,
  pid   = {Lic},
}




@Book{LindholmWLS:2020,
  author   = {Andreas Lindholm and Niklas Wahlström and Fredrik Lindsten and Thomas B. Schön},
  title    = {Machine Learning: A First Course for Engineers and Scientists},
  doi      = {10.1017/9781108919371},
  publisher= {Cambridge University Press},
  year     = {2022},
  place    = {Cambridge},
  url      = {http://smlbook.org/},
  pid      = {B1},
}






@Article{NaessethLS:2019a,
  author   = {Christian A. Naesseth and Fredrik Lindsten and Thomas B. Sch\"on},
  title    = {Elements of Sequential {M}onte {C}arlo},
  journal  = jour_fntml,
  year     = {2019},
  volume   = {12},
  number   = {3},
  pages    = {307--392},
  abstract = {A core problem in statistics and probabilistic machine learning is to compute probability distributions and expectations. This is the fundamental problem of Bayesian statistics and machine learning, which frames all inference as expectations with respect to the posterior distribution. The key challenge is to approximate these intractable expectations. In this tutorial, we review sequential Monte Carlo (SMC), a random-sampling-based class of methods for approximate inference. First, we explain the basics of SMC, discuss practical issues, and review theoretical results. We then examine two of the main user design choices: the proposal distributions and the so called intermediate target distributions. We review recent results on how variational inference and amortization can be used to learn efficient proposals and target distributions. Next, we discuss the SMC estimate of the normalizing constant, how this can be used for pseudo-marginal inference and inference evaluation. Throughout the tutorial we illustrate the use of SMC on various models commonly used in machine learning, such as stochastic recurrent neural networks, probabilistic graphical models, and probabilistic programs.},
  pid  = {M2},
  doi      = {10.1561/2200000074},
  url      = {https://arxiv.org/abs/1903.04797},
}

@Article{LindstenS:2013,
  author    = {Lindsten, Fredrik and Sch\"on, Thomas B.},
  title     = {Backward simulation methods for {M}onte {C}arlo statistical inference},
  journal   = jour_fntml,
  year      = {2013},
  volume    = {6},
  number    = {1},
  pages     = {1--143},
  doi = {10.1561/2200000045},
  abstract = {Monte Carlo methods, in particular those based on Markov chains and on interacting particle systems, are by now tools that are routinely used in machine learning. These methods have had a profound impact on statistical inference in a wide range of application areas where probabilistic models are used. Moreover, there are many algorithms in machine learning which are based on the idea of processing the data sequentially, first in the forward direction and then in the backward direction. In this tutorial, we will review a branch of Monte Carlo methods based on the forward–backward idea, referred to as backward simulators. These methods are useful for learning and inference in probabilistic models containing latent stochastic processes. The theory and practice of backward simulation algorithms have undergone a significant development in recent years and the algorithms keep finding new applications. The foundation for these methods is sequential Monte Carlo (SMC). SMC-based backward simulators are capable of addressing smoothing problems in sequential latent variable models, such as general, nonlinear/non-Gaussian state-space models (SSMs). However, we will also clearly show that the underlying backward simulation idea is by no means restricted to SSMs. Furthermore, backward simulation plays an important role in recent developments of Markov chain Monte Carlo (MCMC) methods. Particle MCMC is a systematic way of using SMC within MCMC. In this framework, backward simulation gives us a way to significantly improve the performance of the samplers. We review and discuss several related backward-simulation-based methods for state inference as well as learning of static parameters, both using a frequentistic and a Bayesian approach.},
  pid   = {M1},
}


@Article{KonoldEtAl:2024,
  author   = {Patrick Konold and Leonardo Monrroy and Alfredo Bellisario and Diogo Filipe and Patrick Adams and Roberto Alvarez and Richard Bean and Johan Bielecki and Szabolcs Bódizs and Gabriel Ducroq and Helmut Grubmueller and Richard Kirian and Marco Kloos and Jayanath Koliyadu and Faisal Koua and Taru Larkiala and Romain Letrun and Fredrik Lindsten and Michael Maihöfer and Andrew Martin and Petra Mészáros and Jennifer Mutisya and Amke Nimmrich and Kenta Okamoto and Adam Round and Tokushi Sato and Joana Valerio and Daniel Westphal and August Wolter and Tej Yenupuri and Tong You and Filipe Maia and Sebastian Westenhoff},
  title    = {Microsecond time-resolved {X}-ray scattering by utilizing {MHz} repetition rate at second-generation {XFELs}},
  journal  = {Nature Methods},
  year = {2024},
  note = {Forthcoming},  
  pid  = {J19},
}

@Article{ZimmermannLMN:2023,
  author   = {Heiko Zimmermann and Fredrik Lindsten and Jan-Willem van de Meent and Christian A Naesseth},
  title    = {A Variational Perspective on Generative Flow Networks},
  journal  = {Transactions on Machine Learning Research (TMLR)},
  year = {2023},
  month = {Apr},
  url = {https://openreview.net/forum?id=AZ4GobeSLq},
  pid  = {J18},
}


@Article{EkstromAL:2022,
  author   = {Filip {Ekstr\"om Kelvinius} and Fredrik Lindsten and Rickard Armiento},
  title    = {Graph-based machine learning beyond stable materials and relaxed crystal structures},
  journal  = {Physical Review Materials},
  volume = {6},
  issue = {3},
  pages = {033801},
  numpages = {10},
  year = {2022},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevMaterials.6.033801},
  url = {https://arxiv.org/abs/2107.00493},
  pid  = {J17},
}


@Article{WigrenWLWS:2022,
  author   = {Anna Wigren and Johan Wågberg and Fredrik Lindsten and Adrian Wills and Thomas B. Schön},
  title    = {Nonlinear System Identification - Learning while respecting physical models using Sequential {M}onte {C}arlo},
  journal  = {{IEEE} Control Systems Magazine},
  year     = {2022},
  volume = {42},
  number = {1},
  pages = {75--102},
  doi = {https://doi.org/10.1109/MCS.2021.3122269},
  pid  = {J16},
}



@Misc{AlenlovDL:2021,
  author       = {Johan Alenlöv and Arnaud Doucet and Fredrik Lindsten},
  title        = {Pseudo-Marginal {H}amiltonian {M}onte {C}arlo},
  journal      = jour_jmlr,  
  url = {https://www.jmlr.org/papers/v22/19-486.html},
  year         = {2021},
  volume = {22},
  number = {141},
  pages = {1--45},
  abstract     = {Bayesian inference in the presence of an intractable likelihood function is computationally challenging. When following a Markov chain Monte Carlo (MCMC) approach to approximate the posterior distribution in this context, one typically either uses MCMC schemes which target the joint posterior of the parameters and some auxiliary latent variables, or pseudo-marginal Metropolis–Hastings (MH) schemes. The latter mimic a MH algorithm targeting the marginal posterior of the parameters by approximating unbiasedly the intractable likelihood. However, in scenarios where the parameters and auxiliary variables are strongly correlated under the posterior and/or this posterior is multimodal, Gibbs sampling or Hamiltonian Monte Carlo (HMC) will perform poorly and the pseudo-marginal MH algorithm, as any other MH scheme, will be inefficient for high-dimensional parameters. We propose here an original MCMC algorithm, termed pseudo-marginal HMC, which combines the advantages of both HMC and pseudo-marginal schemes. Specifically, the PM-HMC method is controlled by a precision parameter N, controlling the approximation of the likelihood and, for any N, it samples the marginal posterior of the parameters. Additionally, as N tends to infinity, its sample trajectories and acceptance probability converge to those of an ideal, but intractable, HMC algorithm which would have access to the intractable likelihood and its gradient. We demonstrate through experiments that PM-HMC can outperform significantly both standard HMC and pseudo-marginal MH schemes.},
  pid      = {J15},
}



@Article{NaessethLS:2019,
  author  = {Christian A. Naesseth and Fredrik Lindsten and Thomas B. Schön},
  title   = {High-dimensional Filtering using Nested Sequential {M}onte {C}arlo},
  journal = jour_tsp,
  year    = {2019},
  volume  = {67},
  number  = {16},
  pages   = {4177--4188},
  doi     = {10.1109/TSP.2019.2926035},
  url     = {https://arxiv.org/abs/1612.09162},
  abstract = {Sequential Monte Carlo (SMC) methods comprise one of the most successful approaches to approximate Bayesian filtering. However, SMC without good proposal distributions struggle in high dimensions. We propose nested sequential Monte Carlo (NSMC), a methodology that generalises the SMC framework by requiring only approximate, properly weighted, samples from the SMC proposal distribution, while still resulting in a correct SMC algorithm. This way we can exactly approximate the locally optimal proposal, and extend the class of models for which we can perform efficient inference using SMC. We show improved accuracy over other state-of-the-art methods on several spatio-temporal state space models.},
  pid = {J14},
}

@Article{RisuleoLH:2019,
  author  = {Riccardo S. Risuleo and Fredrik Lindsten and Håkan Hjalmarsson},
  title   = {Bayesian nonparametric identification of {W}iener systems},
  journal = {Automatica},
  year    = {2019},
  volume  = {108},
  note    = {Brief paper},
  doi = {10.1016/j.automatica.2019.06.032},
  abstract = {We propose a nonparametric approach for the identification of Wiener systems. We model the impulse response of the linear block and the static nonlinearity using Gaussian processes. The hyperparameters of the Gaussian processes are estimated using an iterative algorithm based on stochastic approximation expectation–maximization. In the iterations, we use elliptical slice sampling to approximate the posterior distribution of the impulse response and update the hyperparameter estimates. The same sampling is finally used to sample the posterior distribution and to compute point estimates. We compare the proposed approach with a parametric approach and a semi-parametric approach. In particular, we show that the proposed method has an advantage when a parametric model for the system is not readily available.},
  pid = {J13},
}

@Article{JacobLS:2019,
  author  = {Pierre E. Jacob and Fredrik Lindsten and Thomas B. Schön},
  title   = {Smoothing with Couplings of Conditional Particle Filters},
  journal = jour_jasa,
  year    = {2019},
  pages   = {721--729},
  volume  = {115},
  number  = {530},
  abstract = {In state–space models, smoothing refers to the task of estimating a latent stochastic process given noisy measurements related to the process. We propose an unbiased estimator of smoothing expectations. The lack-of-bias property has methodological benefits: independent estimators can be generated in parallel, and CI can be constructed from the central limit theorem to quantify the approximation error. To design unbiased estimators, we combine a generic debiasing technique for Markov chains, with a Markov chain Monte Carlo algorithm for smoothing. The resulting procedure is widely applicable and we show in numerical experiments that the removal of the bias comes at a manageable increase in variance. We establish the validity of the proposed estimators under mild assumptions. Numerical experiments are provided on toy models, including a setting of highly informative observations, and for a realistic Lotka–Volterra model with an intractable transition density.},
  pid = {J12},
  doi     = {https://doi.org/10.1080/01621459.2018.1548856},
  url = {https://arxiv.org/abs/1701.02002},
}

@Article{CalafatWLWF:2018,
  author  = {Francisco M. Calafat and Thomas Wahl and Fredrik Lindsten and Joanne Williams and Eleanor Frajka-Williams},
  title   = {Coherent modulation of the sea-level annual cycle in the {U}nited {S}tates by {A}tlantic {R}ossby waves},
  journal = {Nature Communications},
  year    = {2018},
  volume  = {9},
  number  = {2571},
  doi = {10.1038/s41467-018-04898-y},
  abstract = {Changes in the sea-level annual cycle (SLAC) can have profound impacts on coastal areas, including increased flooding risk and ecosystem alteration, yet little is known about the magnitude and drivers of such changes. Here we show, using novel Bayesian methods, that there are significant decadal fluctuations in the amplitude of the SLAC along the United States Gulf and Southeast coasts, including an extreme event in 2008–2009 that is likely (probability ≥68%) unprecedented in the tide-gauge record. Such fluctuations are coherent along the coast but decoupled from deep-ocean changes. Through the use of numerical and analytical ocean models, we show that the primary driver of these fluctuations involves incident Rossby waves that generate fast western-boundary waves. These Rossby waves project onto the basin-wide upper mid-ocean transport (top 1000 m) leading to a link with the SLAC, wherein larger SLAC amplitudes coincide with enhanced transport variability.},
  pid = {J11},
}

@Article{SchonSML:2018,
  author  = {Thomas B. Schön and Andreas Svensson and Lawrence Murray and Fredrik Lindsten},
  title   = {Probabilistic learning of nonlinear dynamical systems using sequential {M}onte {C}arlo},
  journal = {Mechanical Systems and Signal Processing},
  year    = {2018},
  volume  = {104},
  pages   = {866--883},
  abstract = {Probabilistic modeling provides the capability to represent and manipulate uncertainty in data, models, predictions and decisions. We are concerned with the problem of learning probabilistic models of dynamical systems from measured data. Specifically, we consider learning of probabilistic nonlinear state-space models. There is no closed-form solution available for this problem, implying that we are forced to use approximations. In this tutorial we will provide a self-contained introduction to one of the state-of-the-art methods---the particle Metropolis--Hastings algorithm---which has proven to offer a practical approximation. This is a Monte Carlo based method, where the particle filter is used to guide a Markov chain Monte Carlo method through the parameter space. One of the key merits of the particle Metropolis--Hastings algorithm is that it is guaranteed to converge to the "true solution" under mild assumptions, despite being based on a particle filter with only a finite number of particles. We will also provide a motivating numerical example illustrating the method using a modeling language tailored for sequential Monte Carlo methods. The intention of modeling languages of this kind is to open up the power of sophisticated Monte Carlo methods---including particle Metropolis--Hastings---to a large group of users without requiring them to know all the underlying mathematical details.},
  url = {https://arxiv.org/abs/1703.02419},
  pid = {J10},
}

@Article{SvenssonSL:2018,
  author  = {Andreas Svensson and Thomas B. Schön and Fredrik Lindsten},
  title   = {Learning of state-space models with highly informative observations: a tempered Sequential {M}onte {C}arlo solution},
  journal = {Mechanical Systems and Signal Processing},
  year    = {2018},
  volume  = {104},
  pages   = {915--928},
  url = {https://arxiv.org/abs/1702.01618},
  abstract = {Probabilistic (or Bayesian) modeling and learning offers interesting possibilities for systematic representation of uncertainty using probability theory. However, probabilistic learning often leads to computationally challenging problems. Some problems of this type that were previously intractable can now be solved on standard personal computers thanks to recent advances in Monte Carlo methods. In particular, for learning of unknown parameters in nonlinear state-space models, methods based on the particle filter (a Monte Carlo method) have proven very useful. A notoriously challenging problem, however, still occurs when the observations in the state-space model are highly informative, i.e. when there is very little or no measurement noise present, relative to the amount of process noise. The particle filter will then struggle in estimating one of the basic components for probabilistic learning, namely the likelihood p(data|parameters). To this end we suggest an algorithm which initially assumes that there is substantial amount of artificial measurement noise present. The variance of this noise is sequentially decreased in an adaptive fashion such that we, in the end, recover the original problem or possibly a very close approximation of it. The main component in our algorithm is a sequential Monte Carlo (SMC) sampler, which gives our proposed method a clear resemblance to the SMC^2 method. Another natural link is also made to the ideas underlying the approximate Bayesian computation (ABC). We illustrate it with numerical examples, and in particular show promising results for a challenging Wiener-Hammerstein benchmark problem.},
  pid = {J9},
}


@Article{SinghLM:2017,
  author  = {Singh, Sumeetpal S. and Lindsten, Fredrik and Moulines, Eric},
  title   = {Blocking Strategies and Stability of Particle {G}ibbs Samplers},
  journal = {Biometrika},
  year    = {2017},
  volume  = {104},
  number  = {4},
  pages   = {953--969},
  url = {https://arxiv.org/abs/1509.08362},
  doi = {10.1093/biomet/asx051},
  abstract = {Sampling from the conditional (or posterior) probability distribution of the latent states of a Hidden Markov Model, given the realization of the observed process, is a non-trivial problem in the context of Markov Chain Monte Carlo. To do this Andrieu et al. (2010) constructed a Markov kernel which leaves this conditional distribution invariant using a Particle Filter. From a practitioner's point of view, this Markov kernel attempts to mimic the act of sampling all the latent state variables as one block from the posterior distribution but for models where exact simulation is not possible. There are some recent theoretical results that establish the uniform ergodicity of this Markov kernel and that the mixing rate does not diminish provided the number of particles grows at least linearly with the number of latent states in the posterior. This gives rise to a cost, per application of the kernel, that is quadratic in the number of latent states which could be prohibitive for long observation sequences. We seek to answer an obvious but important question: is there a different implementation with a cost per-iteration that grows linearly with the number of latent states, but which is still stable in the sense that its mixing rate does not deteriorate? We address this problem using blocking strategies, which are easily parallelizable, and prove stability of the resulting sampler.},
  pid = {J8},
}

@Article{LindstenJNKSAB:2017,
  author  = {Lindsten, Fredrik and Johansen, Adam and Naesseth, Christian A. and Kirkpatrick, Brent and Sch\"on, Thomas B. and Aston, John and Bouchard-C\^ot\'e, Alexandre},
  title   = {Divide-and-Conquer with Sequential {M}onte {C}arlo},
  journal = jour_jcgs,
  year    = {2017},
  volume  = {26},
  number  = {2},
  pages   = {445--458},
  url = {https://arxiv.org/abs/1406.4993},
  doi = {10.1080/10618600.2016.1237363},
  abstract = {We propose a novel class of Sequential Monte Carlo (SMC) algorithms, appropriate for inference in probabilistic graphical models. This class of algorithms adopts a divide-and-conquer approach based upon an auxiliary tree-structured decomposition of the model of interest, turning the overall inferential task into a collection of recursively solved sub-problems. The proposed method is applicable to a broad class of probabilistic graphical models, including models with loops. Unlike a standard SMC sampler, the proposed Divide-and-Conquer SMC employs multiple independent populations of weighted particles, which are resampled, merged, and propagated as the method progresses. We illustrate empirically that this approach can outperform standard methods in terms of the accuracy of the posterior expectation and marginal likelihood approximations. Divide-and-Conquer SMC also opens up novel parallel implementation options and the possibility of concentrating the computational effort on the most challenging sub-problems. We demonstrate its performance on a Markov random field and on a hierarchical logistic regression problem.},
  pid = {J7},
}

@Article{LindstenBSSG:2016,
  author  = {Fredrik Lindsten and Pete Bunch and S. Särkkä and Thomas B. Schön and Simon J. Godsill},
  title   = {Rao-{B}lackwellized particle smoothers for conditionally linear {G}aussian models},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  year    = {2016},
  volume  = {10},
  number  = {2},
  pages   = {353--365},
  url = {https://arxiv.org/abs/1505.06357},
  abstract = {Sequential Monte Carlo (SMC) methods, such as the particle filter, are by now one of the standard computational techniques for addressing the filtering problem in general state-space models. However, many applications require post-processing of data offline. In such scenarios the smoothing problem--in which all the available data is used to compute state estimates--is of central interest. We consider the smoothing problem for a class of conditionally linear Gaussian models. We present a forward-backward-type Rao-Blackwellized particle smoother (RBPS) that is able to exploit the tractable substructure present in these models. Akin to the well known Rao-Blackwellized particle filter, the proposed RBPS marginalizes out a conditionally tractable subset of state variables, effectively making use of SMC only for the "intractable part" of the model. Compared to existing RBPS, two key features of the proposed method are: (i) it does not require structural approximations of the model, and (ii) the aforementioned marginalization is done both in the forward direction and in the backward direction.},
  pid = {J6},
}

@Article{LindstenDM:2015,
  author    = {Lindsten, Fredrik and Douc, Randal and Moulines, Eric},
  title     = {Uniform ergodicity of the Particle {G}ibbs sampler},
  journal   = jour_sjos,
  year      = {2015},
  volume    = {42},
  number    = {3},
  pages     = {775--797},
  pid   = {J5},
  doi       = {10.1111/sjos.12136},
  url = {https://arxiv.org/abs/1401.0683},
  abstract = {The particle Gibbs (PG) sampler is a systematic way of using a particle filter within Markov chain Monte Carlo (MCMC). This results in an off-the-shelf Markov kernel on the space of state trajectories, which can be used to simulate from the full joint smoothing distribution for a state space model in an MCMC scheme. We show that the PG Markov kernel is uniformly ergodic under rather general assumptions, that we will carefully review and discuss. In particular, we provide an explicit rate of convergence which reveals that: (i) for fixed number of data points, the convergence rate can be made arbitrarily good by increasing the number of particles, and (ii) under general mixing assumptions, the convergence rate can be kept constant by increasing the number of particles superlinearly with the number of observations. We illustrate the applicability of our result by studying in detail two common state space models with non-compact state spaces.},
}

@Article{OezkanLFG:2015,
  author  = {{\"O}zkan, Emre and Lindsten, Fredrik and Fritsche, Carsten and Gustafsson, Fredrik},
  title   = {Recursive maximum likelihood identification of jump {M}arkov nonlinear systems},
  journal = jour_tsp,
  year    = {2015},
  volume  = {63},
  number  = {3},
  pages   = {754--765},
  url  = {https://arxiv.org/abs/1312.0781},
  abstract = {In this contribution, we present an online method for joint state and parameter estimation in jump Markov non-linear systems (JMNLS). State inference is enabled via the use of particle filters which makes the method applicable to a wide range of non-linear models. To exploit the inherent structure of JMNLS, we design a Rao-Blackwellized particle filter (RBPF) where the discrete mode is marginalized out analytically. This results in an efficient implementation of the algorithm and reduces the estimation error variance. The proposed RBPF is then used to compute, recursively in time, smoothed estimates of complete data sufficient statistics. Together with the online expectation maximization algorithm, this enables recursive identification of unknown model parameters. The performance of the method is illustrated in simulations and on a localization problem in wireless networks using real data.},
  pid = {J4},
}

@Article{DahlinLS:2015,
  author  = {Dahlin, Johan and Lindsten, Fredrik and Sch\"on, Thomas B.},
  title   = {Particle {M}etropolis-{H}astings using gradient and {H}essian information},
  journal = jour_statcomp,
  year    = {2015},
  volume  = {25},
  number  = {1},
  pages   = {81--92},
  url = {https://arxiv.org/abs/1311.0686},
  abstract = {Particle Metropolis-Hastings (PMH) allows for Bayesian parameter inference in nonlinear state space models by combining Markov chain Monte Carlo (MCMC) and particle filtering. The latter is used to estimate the intractable likelihood. In its original formulation, PMH makes use of a marginal MCMC proposal for the parameters, typically a Gaussian random walk. However, this can lead to a poor exploration of the parameter space and an inefficient use of the generated particles. We propose a number of alternative versions of PMH that incorporate gradient and Hessian information about the posterior into the proposal. This information is more or less obtained as a byproduct of the likelihood estimation. Indeed, we show how to estimate the required information using a fixed-lag particle smoother, with a computational cost growing linearly in the number of particles. We conclude that the proposed methods can: (i) decrease the length of the burn-in phase, (ii) increase the mixing of the Markov chain at the stationary phase, and (iii) make the proposal distribution scale invariant which simplifies tuning.},
  pid = {J3},
}

@Article{LindstenJS:2014,
  author    = {Lindsten, Fredrik and Jordan, Michael I. and Sch\"on, Thomas B.},
  title     = {Particle {G}ibbs with Ancestor Sampling},
  journal   = jour_jmlr,
  year      = {2014},
  volume    = {15},
  pages     = {2145--2184},
  url = {http://jmlr.org/papers/v15/lindsten14a.html},
  abstract = {Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combining the two main tools used for Monte Carlo statistical inference: sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). We present a novel PMCMC algorithm that we refer to as particle Gibbs with ancestor sampling (PGAS). PGAS provides the data analyst with an off-the-shelf class of Markov kernels that can be used to simulate the typically high-dimensional and highly autocorrelated state trajectory in a state-space model. The ancestor sampling procedure enables fast mixing of the PGAS kernel even when using seemingly few particles in the underlying SMC sampler. This is important as it can significantly reduce the computational burden that is typically associated with using SMC. PGAS is conceptually similar to the existing PG with backward simulation (PGBS) procedure. Instead of using separate forward and backward sweeps as in PGBS, however, we achieve the same effect in a single forward sweep. This makes PGAS well suited for addressing inference problems not only in state-space models, but also in models with more complex dependencies, such as non-Markovian, Bayesian nonparametric, and general probabilistic graphical models.},
  pid   = {J2},
}



@Article{LindstenSJ:2013,
  author    = {Lindsten, Fredrik and Sch\"on, Thomas B. and Jordan, Michael I.},
  title     = {Bayesian semiparametric {W}iener system identification},
  journal   = jour_automatica,
  year      = {2013},
  volume    = {49},
  number    = {7},
  pages     = {2053--2063},
  doi = {10.1016/j.automatica.2013.03.021},
  abstract = {We present a novel method for Wiener system identification. The method relies on a semiparametric, i.e. a mixed parametric/nonparametric, model of a Wiener system. We use a state-space model for the linear dynamical system and a nonparametric Gaussian process model for the static nonlinearity. We avoid making strong assumptions, such as monotonicity, on the nonlinear mapping. Stochastic disturbances, entering both as measurement noise and as process noise, are handled in a systematic manner. The nonparametric nature of the Gaussian process allows us to handle a wide range of nonlinearities without making problem-specific parameterizations. We also consider sparsity-promoting priors, based on generalized hyperbolic distributions, to automatically infer the order of the underlying dynamical system. We derive an inference algorithm based on an efficient particle Markov chain Monte Carlo method, referred to as particle Gibbs with ancestor sampling. The method is profiled on two challenging identification problems with good results. Blind Wiener system identification is handled as a special case.},
  pid   = {J1},
}

